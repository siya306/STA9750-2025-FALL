[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project #01",
    "section": "",
    "text": "Executive Summary\nThis research investigates the Netflix Top 10 dataset, concentrating on how episodes and films performed in various countries. I used R and tidyverse tools to wrangle, filter, and summarise data in order to answer important analytical questions. The study focuses on patterns such as the longest-running series, debut week performances, and US film rankings. The analysis illustrates not only raw data insights, but also how to effectively explain outcomes. All code is integrated to ensure transparency and repeatability.\n\n\nIntroduction\nNetflix has grown into a global streaming powerhouse, producing and distributing content in more than 90 countries. This dataset, which records weekly Top 10 rankings, provides insights into which shows and films are most popular around the world.\nThe project‚Äôs purpose is to use R to answer a series of structured questions. These include determining the longest-running shows on Top 10 lists, highlighting exceptional premiere weeks, and evaluating trends in specific areas such as the United States.\n\n\n\nNetflix Logo\n\n\n\n\n\n1. Acquire Data\nThe datasets used include:\n\nGlobal Top 10: weekly Top 10 shows and films worldwide.\n\nCountry Top 10: weekly Top 10 shows and films per country.\n\n\n\nCode\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\n\nStoring the data locally makes sure that it can be used again. Anyone who runs this report again can automatically get the right datasets if they are missing.\n\n\n2. Data Import and Preparation\n\n\nCode\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n\nLoading required package: tidyverse\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\n\n\nGLOBAL_TOP_10 &lt;- read_tsv(\"data/mp01/global_top10_alltime.csv\")\n\n\nRows: 8880 Columns: 9\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(season_title = if_else(season_title == \"N/A\", NA_character_, season_title))\n\n\nstr(GLOBAL_TOP_10)\n\n\ntibble [8,880 √ó 9] (S3: tbl_df/tbl/data.frame)\n $ week                      : Date[1:8880], format: \"2025-09-28\" \"2025-09-28\" ...\n $ category                  : chr [1:8880] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8880] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8880] \"KPop Demon Hunters\" \"Ruth & Boaz\" \"The Wrong Paris\" \"Man on Fire\" ...\n $ season_title              : chr [1:8880] NA NA NA NA ...\n $ weekly_hours_viewed       : num [1:8880] 32200000 15900000 13500000 15700000 11200000 8400000 6800000 6200000 4900000 8400000 ...\n $ runtime                   : num [1:8880] 1.67 1.55 1.78 2.43 1.83 ...\n $ weekly_views              : num [1:8880] 19300000 10300000 7600000 6500000 6100000 4900000 3600000 3200000 3200000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8880] 15 1 3 5 2 1 1 1 1 3 ...\n\n\nCode\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,880\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0‚Ä¶\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film‚Ä¶\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, ‚Ä¶\n$ show_title                 &lt;chr&gt; \"KPop Demon Hunters\", \"Ruth & Boaz\", \"The W‚Ä¶\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"aka Ch‚Ä¶\n$ weekly_hours_viewed        &lt;dbl&gt; 32200000, 15900000, 13500000, 15700000, 112‚Ä¶\n$ runtime                    &lt;dbl&gt; 1.6667, 1.5500, 1.7833, 2.4333, 1.8333, 1.7‚Ä¶\n$ weekly_views               &lt;dbl&gt; 19300000, 10300000, 7600000, 6500000, 61000‚Ä¶\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 15, 1, 3, 5, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, ‚Ä¶\n\n\nCode\nCOUNTRY_TOP_10 &lt;- read_tsv(\n  \"data/mp01/country_top10_alltime.csv\",\n  na = \"N/A\"\n)\n\n\nRows: 413620 Columns: 8\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nstr(COUNTRY_TOP_10)\n\n\nspc_tbl_ [413,620 √ó 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country_name              : chr [1:413620] \"Argentina\" \"Argentina\" \"Argentina\" \"Argentina\" ...\n $ country_iso2              : chr [1:413620] \"AR\" \"AR\" \"AR\" \"AR\" ...\n $ week                      : Date[1:413620], format: \"2025-09-28\" \"2025-09-28\" ...\n $ category                  : chr [1:413620] \"Films\" \"Films\" \"Films\" \"Films\" ...\n $ weekly_rank               : num [1:413620] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:413620] \"Sonic the Hedgehog 3\" \"KPop Demon Hunters\" \"French Lover\" \"She Said Maybe\" ...\n $ season_title              : chr [1:413620] NA NA NA NA ...\n $ cumulative_weeks_in_top_10: num [1:413620] 2 15 1 2 1 1 2 5 1 2 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country_name = col_character(),\n  ..   country_iso2 = col_character(),\n  ..   week = col_date(format = \"\"),\n  ..   category = col_character(),\n  ..   weekly_rank = col_double(),\n  ..   show_title = col_character(),\n  ..   season_title = col_character(),\n  ..   cumulative_weeks_in_top_10 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nCode\nglimpse(COUNTRY_TOP_10)\n\n\nRows: 413,620\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg‚Ä¶\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"‚Ä¶\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0‚Ä¶\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"‚Ä¶\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, ‚Ä¶\n$ show_title                 &lt;chr&gt; \"Sonic the Hedgehog 3\", \"KPop Demon Hunters‚Ä¶\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Bi‚Ä¶\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 15, 1, 2, 1, 1, 2, 5, 1, 2, 2, 1, 1, 1, ‚Ä¶\n\n\n\nFor a better analysis, missing numbers like ‚ÄúN/A‚Äù are changed to ‚ÄúNA.‚Äù\nThe global dataset has both movies and TV shows, and the country dataset lets us see how well shows did in different areas.\n\n\n\n3. Initial Data Exploration\n\n\n\nNetflix Logo\n\n\n\n\nCode\nlibrary(DT)\nGLOBAL_TOP_10 |&gt; \n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\n\n\nCode\nlibrary(stringr)\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))  \n\n\n\n\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n    select(-season_title) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))    \n\n\n\n\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))    \n\n\n\n\n\n\nA first look at the dataset reveals key variables: show title, category, weekly hours viewed, runtime, and cumulative weeks in Top 10. This allows us to understand the dataset before deeper analysis.\n\n\nExploratory Questions\n\n\n1. How many different countries does Netflix operate in?\n\n\nCode\nlibrary(dplyr)\nnum_countries &lt;- COUNTRY_TOP_10 %&gt;%\n  distinct(country_name) %&gt;%\n  nrow()\nnum_countries\n\n\n[1] 94\n\n\nThe number of countries Netflix operates in is 94.\nThis shows how global Netflix is and how well it has localized content for a wide range of audiences.\n\n\n2. Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\nCode\nnon_english_top &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(!str_detect(category,\"Films English\")) %&gt;%\n  arrange(desc(cumulative_weeks_in_top_10)) %&gt;%\n  slice_head(n = 1) %&gt;%\n  select(show_title, cumulative_weeks_in_top_10)\n\nnon_english_top\n\n\n# A tibble: 1 √ó 2\n  show_title cumulative_weeks_in_top_10\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 Squid Game                         32\n\n\nThe non-English-language film that spent the most cumulative weeks in the global Top 10 is Squid Game, with 32 weeks.\nNon-English hits show how Netflix is trying to diversify its content around the world. Regional films that do well on global charts show that they appeal to everyone.\n\n\n3. What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\n\nCode\nlongest_film &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(runtime_minutes = round(runtime * 60)) %&gt;%\n  arrange(desc(runtime_minutes)) %&gt;%\n  slice_head(n = 1) %&gt;%\n  select(show_title, runtime_minutes)\n\nlongest_film\n\n\n# A tibble: 1 √ó 2\n  show_title     runtime_minutes\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Until You Burn            2855\n\n\nThe longest film to appear in the global Top 10 is Until You Burn, with a runtime of 2855 minutes.\nNon-English hits demonstrate Netflix‚Äôs strategy of global content diversification. Success in global charts proves the universal appeal of regional films.\n\n\n4. For each of the four categories, what program has the most total hours of global viewership?\n\n\nCode\ntop_viewed_per_category &lt;- GLOBAL_TOP_10 %&gt;%\n  group_by(category, show_title) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(category) %&gt;%\n  slice_max(total_hours, n = 1, with_ties = FALSE) %&gt;%\n  arrange(category)\n\ntop_viewed_per_category\n\n\n# A tibble: 4 √ó 3\n# Groups:   category [4]\n  category            show_title          total_hours\n  &lt;chr&gt;               &lt;chr&gt;                     &lt;dbl&gt;\n1 Films (English)     KPop Demon Hunters    591300000\n2 Films (Non-English) Society of the Snow   235900000\n3 TV (English)        Stranger Things      2967980000\n4 TV (Non-English)    Squid Game           5048300000\n\n\n\n\nCode\nlibrary(DT)\ntop_viewed_per_category %&gt;%\n  datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\n\n\n5. Which TV show had the longest run in a country‚Äôs Top 10? How long and where?\n\n\nCode\nlibrary(dplyr)\nlibrary(DT)\n\nlongest_run &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(cumulative_weeks_in_top_10 == max(cumulative_weeks_in_top_10, na.rm = TRUE)) %&gt;%\n  select(show_title, country_name, cumulative_weeks_in_top_10) %&gt;%\n  distinct()\n\n# Display as interactive table\ndatatable(longest_run, \n          options = list(searching = FALSE, info = FALSE),\n          rownames = FALSE)\n\n\n\n\n\n\nThis is an example of how local favorites can shape cultural watching habits for years.\n\n\n6. Netflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\n\ncountry_short_history &lt;- COUNTRY_TOP_10 %&gt;%\n  group_by(country_name) %&gt;%\n  summarise(\n    total_weeks = n_distinct(week),    # count unique weeks, not rows\n    last_week = max(week)\n  ) %&gt;%\n  filter(total_weeks &lt; 200)\n\ncountry_short_history\n\n\n# A tibble: 1 √ó 3\n  country_name total_weeks last_week \n  &lt;chr&gt;              &lt;int&gt; &lt;date&gt;    \n1 Russia                35 2022-02-27\n\n\nThe country with less than 200 weeks of Netflix service is Russia, which ceased operations on 2022-02-27.\nThis suggests Netflix exited that market, possibly due to business or regulatory challenges.\n\n\n7. What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\n\nCode\nsquid_game_hours &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Squid Game\")) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nsquid_game_hours\n\n\n# A tibble: 1 √ó 1\n  total_hours\n        &lt;dbl&gt;\n1  5310000000\n\n\nThe total global viewership of Squid Game across all seasons is 5.31^{9} hours.\nThis number illustrates Netflix‚Äôs biggest-ever hit, validating investments in international originals.\n\n\n8. The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\n\nCode\nred_notice_views &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Red Notice\") & year(week) == 2021) %&gt;%\n  mutate(views_estimate = weekly_hours_viewed / (1 + 58/60)) %&gt;%\n  summarise(total_views = sum(views_estimate, na.rm = TRUE))\n\nred_notice_views\n\n\n# A tibble: 1 √ó 1\n  total_views\n        &lt;dbl&gt;\n1  201732203.\n\n\nRed Notice received approximately 2.017322^{8} views in 2021.\nThis calculation uses runtime to estimate views, showing how viewership estimates can be derived indirectly when raw counts aren‚Äôt given.\n\n\n9. How many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\n\n# Filter US films and summarize\nus_films &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_iso2 == \"US\", str_detect(category, \"Films\")) %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(\n    first_rank = min(weekly_rank, na.rm = TRUE),\n    ever_number1 = any(weekly_rank == 1, na.rm = TRUE),\n    last_week = max(week, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(first_rank &gt; 1 & ever_number1) %&gt;%\n  arrange(desc(last_week))\n\n# Number of films\nnum_us_films &lt;- nrow(us_films)\n\n# Most recent film to reach #1\nmost_recent_film &lt;- us_films$show_title[1]\n\n# Inline code in Quarto\nnum_us_films   # use in text: `r num_us_films`\n\n\n[1] 0\n\n\nCode\nmost_recent_film  # use in text: `r most_recent_film`\n\n\n[1] NA\n\n\nThe number of Films that reached Number 1 in the US but did not debut there is 0.\n\n\n10. Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\n\ntop_show &lt;- COUNTRY_TOP_10 %&gt;%\n  group_by(show_title, country_name) %&gt;%\n  summarise(debut_week = min(week), .groups = \"drop\") %&gt;%\n  group_by(show_title, debut_week) %&gt;%\n  summarise(num_countries = n(), .groups = \"drop\") %&gt;%\n  slice_max(num_countries, n = 1) %&gt;% \n  slice(1)                             \n\ntop_show$show_title   \n\n\n[1] \"Army of Thieves\"\n\n\nCode\ntop_show$num_countries  \n\n\n[1] 94\n\n\nThe TV show/season that hit the top 10 in the most countries in its debut week is Army of Thieves, appearing in 94 countries.\nThis reflects a global marketing push or universal story appeal, making the show an instant worldwide hit.\n\n\nConclusion\nIn conclusion, this project used coding, statistical analysis, and data visualization to find important information on how Netflix‚Äôs global Top 10 movies and shows did. We were able to clean, investigate, and analyze massive datasets by working through R and Quarto in a methodical way. This helped us answer concerns regarding international reach, the dominance of non-English titles, and the record-breaking runs of popular shows and movies. The code not only made it easier to do tedious math, including figuring out how many views a video gets, tracking how well a video does in different regions, and counting total views, but it also made the analysis clear and easy to repeat. Cultural events like Squid Game changed the way people around the world enjoy entertainment, while movies like Red Notice highlighted how big Netflix‚Äôs star-driven initiatives are. At the same time, regional trends and shows that slowly ascended to the top showed how different people like different things. This mix of coding and storytelling shows how data science can turn raw streaming data into a story about how Netflix has changed how people watch TV around the world.\n\n\nPress-Release\n\nPress Release 1: Upcoming Season of Stranger Things\n\n\nThe Grand Finale ‚ÄúThe Upside Down Returns: Stranger Things Ends With a Global Spectacle‚Äù.\n\n\n\nNetflix Logo\n\n\nThe fifth and last season of Stranger Things will come out on Netflix at the end of 2025. This will be the end of one of the platform‚Äôs most popular shows. Stranger Things has had an amazing 2.97 billion hours of global viewership throughout its first four seasons. It has been in the Top 10 for 366 weeks in a row and has charted in 94 countries around the world, making it a hit in every area where Netflix is available. It is more popular than other English-language TV shows on Netflix because it is so popular around the world and has been for so long. This illustrates that the series is a cultural phenomenon and a key part of the streaming service‚Äôs global success. Fans all over the world are excited for the finale, which promises to contain the same thrilling suspense, emotion, and ntalgia that have made the show a hit across the world.\n\n\n\n\n\n\n\n\n\nFun Fact: Fans around the world have watched Stranger Things for more than 2.97 billion hours, which is enough time to watch all the seasons more than 1,000 times!\n\n\nPress Release 2: Commercial Success in India\n\n\nIndian Netflix Takes Off with Hindi Hits and Millions of New Viewers.\nIndia, the country with the most people in the world, has become an important growth area for Netflix since Hindi-language movies and TV shows are becoming more popular. RRR (Hindi) was at the top of India‚Äôs Top 10 for 325 weeks, while Kantara (Hindi) stayed strong for 66 weeks. Fabulous Lives of Bollywood Wives, Hi Papa (Hindi), and Sir (Hindi) were all big hits with audiences, each staying popular for more than 25 weeks. In 2024, Hindi viewership frequently went over 15 million hours a week, adding up to 183 million hours for the year. This means that there are about 730,000 active users in India, which shows how important the region is becoming as Netflix‚Äôs next big thing. Long-term trends demonstrate that Hindi content is not only getting more people to watch it, but it‚Äôs also keeping them interested for years. The number of Hindi movies in India‚Äôs Top 10 has been continuously rising, their average runs are getting longer, and their watching peaks are getting higher with stronger weekly baselines. These trends suggest that Netflix will continue to gain subscribers in India, which is an important part of the company‚Äôs worldwide growth strategy.\n\n\n\n\n\n\n\n\n\nFun Fact: RRR (Hindi) alone spent almost 5.5 years in India‚Äôs Top 10 charts, which is longer than most movies stay in theaters around the world!\n\n\n\n\n\n\n\n\n\n\n\nPress Release 3: OWN TOPIC\n\n\nNetflix Celebrates the Global Success of Non-English Originals.\n\n\n\nNetflix Logo\n\n\nNetflix keeps breaking down barriersby making non-English TV shows and movies popular all around the world. This shows that great tales can be told in any language. Squid Game is a landmark achievement with over 5.31 billion hours seen worldwide and 32 weeks in the Global Top 10. Money Heist, a Spanish-language hit, spent an incredible 127 weeks in a single country‚Äôs Top 10 (Pakistan), showing how long it can stay popular. Hindi hits like RRR (Hindi) and Kantara (Hindi) kept this trend going, showing how India‚Äôs influence on global viewing patterns is growing. Netflix is putting more money into local productions that create global cultural moments and help the company build its subscriber base over time. This is because non-English originals always do well in different locations.\n\n\n\n\n\n\n\n\n\nFun Fact:Squid Game alone had more viewers around the world than many Hollywood blockbusters combined, with over 5.3 billion hours viewed!\n\n\n\n\n\n\n\n\n\nFun Fact: By the end of 2024, almost half of all Netflix viewing hours came from titles that weren‚Äôt in English. This shows that people‚Äôs tastes had changed around the world.\n\n\n\nREFERENCE CODES FOR PRESS RELEASES:\n\nPRESS RELEASE 1\n\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\n\nstranger_things_global &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Stranger Things\")) %&gt;%\n  summarise(\n    total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n    total_weeks_in_top10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE)\n  )\n\nstranger_things_countries &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Stranger Things\")) %&gt;%\n  group_by(country_name) %&gt;%\n  summarise(debut_week = min(week), .groups = \"drop\") %&gt;%\n  summarise(num_countries = n())\n\nstranger_things_stats &lt;- cbind(stranger_things_global, stranger_things_countries)\nstranger_things_stats\n\n\n  total_hours total_weeks_in_top10 num_countries\n1  2967980000                  366            93\n\n\n\nPRESS RELEASE 2\n\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\n\nindia_hindi &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_iso2 == \"IN\", str_detect(show_title, \"Hindi|Bollywood\")) %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(\n    total_weeks_in_top10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(total_weeks_in_top10))\n\n\ntop_india_hindi &lt;- india_hindi %&gt;% slice_head(n = 5)\ntop_india_hindi\n\n\n# A tibble: 5 √ó 2\n  show_title                        total_weeks_in_top10\n  &lt;chr&gt;                                            &lt;dbl&gt;\n1 RRR (Hindi)                                        325\n2 Kantara (Hindi)                                     66\n3 Fabulous Lives of Bollywood Wives                   34\n4 Hi Papa (Hindi)                                     28\n5 Sir (Hindi)                                         28\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\n\nhindi_hours &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"(Hindi)\")) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nhindi_hours\n\n\n# A tibble: 1 √ó 1\n  total_hours\n        &lt;dbl&gt;\n1   183000000\n\n\n\n\nCode\nhindi_total_hours &lt;- 1.83e+08  \n\nassumed_hours &lt;- c(240, 250, 300)\n\nsubs_estimates &lt;- data.frame(\n  hours_per_user_year = assumed_hours,\n  estimated_subscribers = hindi_total_hours / assumed_hours\n)\n\nsubs_estimates\n\n\n  hours_per_user_year estimated_subscribers\n1                 240                762500\n2                 250                732000\n3                 300                610000\n\n\n\nPRESS RELEASE 3\n\n\n\nCode\nlibrary(dplyr); library(stringr)\nsquid_stats &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Squid Game\")) %&gt;%\n  summarise(\n    total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n    weeks_global_top10 = max(cumulative_weeks_in_top_10, na.rm = TRUE)\n  )\nsquid_stats\n\n\n# A tibble: 1 √ó 2\n  total_hours weeks_global_top10\n        &lt;dbl&gt;              &lt;dbl&gt;\n1  5310000000                 32\n\n\n\n\nCode\nmh_long_run &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Money Heist\")) %&gt;%\n  slice_max(cumulative_weeks_in_top_10, n = 1, with_ties = FALSE) %&gt;%\n  select(show_title, country_name, cumulative_weeks_in_top_10)\nmh_long_run\n\n\n# A tibble: 1 √ó 3\n  show_title  country_name cumulative_weeks_in_top_10\n  &lt;chr&gt;       &lt;chr&gt;                             &lt;dbl&gt;\n1 Money Heist Pakistan                            127\n\n\n\n\nCode\nindia_hindi_examples &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\", str_detect(show_title, \"\\\\(Hindi\\\\)\")) %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(weeks_in_top10 = max(cumulative_weeks_in_top_10, na.rm = TRUE), .groups=\"drop\") %&gt;%\n  filter(show_title %in% c(\"RRR (Hindi)\", \"Kantara (Hindi)\"))\nindia_hindi_examples\n\n\n# A tibble: 2 √ó 2\n  show_title      weeks_in_top10\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Kantara (Hindi)             11\n2 RRR (Hindi)                 25\n\n\nTHE END"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 9750 Submission Material",
    "section": "",
    "text": "Welcome to my STA 9750 project site!\nThis site will host my submissions and project materials for the course."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "",
    "text": "Code\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\n\n\n\n\nCode\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\n\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    library(dplyr)\n    library(tidyr)\n    library(readr)\n    \n    if(!file.exists(fname)){\n        \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        # These were looked up manually on bls.gov after finding \n        # they were presented as ranges. Since there are only three\n        # it was easier to manually handle than to special-case everything else\n        naics_missing &lt;- tibble::tribble(\n            ~Code, ~title, ~depth, \n            \"31\", \"Manufacturing\", 1,\n            \"32\", \"Manufacturing\", 1,\n            \"33\", \"Manufacturing\", 1,\n            \"44\", \"Retail\", 1, \n            \"45\", \"Retail\", 1,\n            \"48\", \"Transportation and Warehousing\", 1, \n            \"49\", \"Transportation and Warehousing\", 1\n        )\n        \n        naics_table &lt;- bind_rows(naics_table, naics_missing)\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code) |&gt;\n            drop_na() |&gt;\n            mutate(across(contains(\"code\"), as.integer))\n        \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n\n\n\nCode\n# Create the directory for data if it doesn't already exist\nif (!dir.exists(file.path(\"data\", \"mp02\"))) {\n  dir.create(file.path(\"data\", \"mp02\"), recursive = TRUE, showWarnings = FALSE)\n}\n\n\n\n\nCode\nlist.files(\"data/mp02\")\n\n\n[1] \"B01003_001_cbsa_2009_2023.csv\" \"B11001_001_cbsa_2009_2023.csv\"\n[3] \"B19013_001_cbsa_2009_2023.csv\" \"B25064_001_cbsa_2009_2023.csv\"\n[5] \"bls_industry_codes.csv\"        \"bls_qcew_2009_2023.csv.gz\"    \n[7] \"housing_units_2009_2023.csv\"  \n\n\n\n\n\n\n\n\n\nFigure: Data relationship diagram showing how ACS, BLS, and Census datasets connect via std_cbsa and year."
  },
  {
    "objectID": "mp02.html#extra-credit-opportunity-01-data-relationship-diagram",
    "href": "mp02.html#extra-credit-opportunity-01-data-relationship-diagram",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "1 Extra Credit Opportunity #01 ‚Äî Data Relationship Diagram",
    "text": "1 Extra Credit Opportunity #01 ‚Äî Data Relationship Diagram\n\n\n\n\n\nFigure: Data relationship diagram showing how ACS, BLS, and Census datasets connect via std_cbsa and year."
  },
  {
    "objectID": "mp02.html#data-relationship-diagram",
    "href": "mp02.html#data-relationship-diagram",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "",
    "text": "Figure: Data relationship diagram showing how ACS, BLS, and Census datasets connect via std_cbsa and year."
  },
  {
    "objectID": "mp02.html#task-2",
    "href": "mp02.html#task-2",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "0.2 TASK 2",
    "text": "0.2 TASK 2"
  },
  {
    "objectID": "mp02.html#task-3",
    "href": "mp02.html#task-3",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "5.1 TASK 3",
    "text": "5.1 TASK 3"
  },
  {
    "objectID": "mp02.html#task-4-rent-burden-join-income-rent-build-an-index-and-show-tables",
    "href": "mp02.html#task-4-rent-burden-join-income-rent-build-an-index-and-show-tables",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "8.1 TASK 4 Rent Burden (join INCOME + RENT, build an index, and show tables)",
    "text": "8.1 TASK 4 Rent Burden (join INCOME + RENT, build an index, and show tables)\n\n\nCode\n# Join rent & income by CBSA (GEOID + year), compute rent-to-income (annualized),\n# then standardize to an index where 100 = national average in 2009.\n\nrent_income_all &lt;- RENT |&gt;\n  select(GEOID, NAME, year, monthly_rent) |&gt;\n  left_join(INCOME |&gt; select(GEOID, year, household_income),\n            by = c(\"GEOID\",\"year\")) |&gt;\n  mutate(\n    rti = (monthly_rent * 12) / household_income  # rent-to-income ratio\n  )\n\n# Baseline = national average in 2009\nbaseline_2009 &lt;- rent_income_all |&gt;\n  filter(year == 2009) |&gt;\n  summarise(baseline = mean(rti, na.rm = TRUE)) |&gt;\n  pull(baseline)\n\nrent_burden &lt;- rent_income_all |&gt;\n  mutate(\n    rent_burden_index = 100 * (rti / baseline_2009)  # 100 = 2009 national average\n  )\n\n\nRent Burden Index = 100 √ó (rent-to-income) / (national average in 2009). Values above 100 mean a CBSA is more burdened than the 2009 national average; values below 100 mean less burdened."
  },
  {
    "objectID": "mp02.html#task-5",
    "href": "mp02.html#task-5",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "9.1 TASK 5",
    "text": "9.1 TASK 5\n5A) Join, compute 5-year growth, build metrics\n\n\nCode\n# ---- Task 5: Housing Growth ----\n\n# Prepare pop table to join to PERMITS (PERMITS$CBSA is integer; GEOID is character)\npop_for_join &lt;- POPULATION |&gt;\n  mutate(cbsa_int = as.integer(GEOID)) |&gt;\n  select(cbsa_int, GEOID, NAME, year, population)\n\n# Clean permits\npermits_clean &lt;- PERMITS |&gt;\n  rename(cbsa_int = CBSA, permits = new_housing_units_permitted) |&gt;\n  select(cbsa_int, year, permits)\n\n# Join population + permits by cbsa_int and year\nhousing_base &lt;- pop_for_join |&gt;\n  left_join(permits_clean, by = c(\"cbsa_int\",\"year\"))\n\n# Compute 5-year population growth within CBSA (starts in 2014)\nhousing_base &lt;- housing_base |&gt;\n  group_by(GEOID) |&gt;\n  arrange(year, .by_group = TRUE) |&gt;\n  mutate(\n    population_5y_ago = dplyr::lag(population, 5),\n    pop_growth_5y = population - population_5y_ago\n  ) |&gt;\n  ungroup()\n\n# Metric 1: Instantaneous permits per 1,000 residents (current year)\nhousing_base &lt;- housing_base |&gt;\n  mutate(inst_permits_per_1k = 1000 * permits / population)\n\n# Metric 2: Permits per 1,000 of 5-year population growth (avoid div-by-zero)\nhousing_base &lt;- housing_base |&gt;\n  mutate(\n    rate_permits_per_1k_growth = dplyr::if_else(\n      !is.na(pop_growth_5y) & pop_growth_5y &gt; 0,\n      1000 * permits / pop_growth_5y,\n      NA_real_\n    )\n  )\n\n# Standardize to indices: 100 = overall average (all CBSAs, all years)\ninst_baseline &lt;- mean(housing_base$inst_permits_per_1k, na.rm = TRUE)\nrate_baseline &lt;- mean(housing_base$rate_permits_per_1k_growth, na.rm = TRUE)\n\nhousing_growth &lt;- housing_base |&gt;\n  mutate(\n    inst_index = 100 * inst_permits_per_1k / inst_baseline,\n    rate_index = 100 * rate_permits_per_1k_growth / rate_baseline,\n    composite_index = (inst_index + rate_index) / 2  # equal weights\n  )\n\n\n5B) Tables: Top/Bottom CBSAs (latest year)\n\n\nCode\n# Latest year present in the joined frame\nlatest_year &lt;- max(housing_growth$year, na.rm = TRUE)\n\nhg_latest &lt;- housing_growth |&gt;\n  filter(year == latest_year) |&gt;\n  distinct(GEOID, NAME, year,\n           inst_permits_per_1k, rate_permits_per_1k_growth,\n           inst_index, rate_index, composite_index)\n\n# Top by Instantaneous\nDT::datatable(\n  hg_latest |&gt;\n    arrange(desc(inst_index)) |&gt;\n    transmute(\n      CBSA = NAME,\n      Year = year,\n      `Permits per 1k Residents` = round(inst_permits_per_1k, 2),\n      `Instantaneous Index` = round(inst_index, 1)\n    ) |&gt;\n    head(15),\n  caption = paste0(\"Top CBSAs by Instantaneous Housing Growth (\", latest_year, \")\"),\n  options = list(pageLength = 15, autoWidth = TRUE)\n)\n\n\n\n\n\n\nCode\n# Top by Rate-based (relative to 5-year population growth)\nDT::datatable(\n  hg_latest |&gt;\n    arrange(desc(rate_index)) |&gt;\n    transmute(\n      CBSA = NAME,\n      Year = year,\n      `Permits per 1k of 5y Pop Growth` = round(rate_permits_per_1k_growth, 2),\n      `Rate-based Index` = round(rate_index, 1)\n    ) |&gt;\n    head(15),\n  caption = paste0(\"Top CBSAs by Rate-based Housing Growth (\", latest_year, \")\"),\n  options = list(pageLength = 15, autoWidth = TRUE)\n)\n\n\n\n\n\n\nCode\n# Top by Composite\nDT::datatable(\n  hg_latest |&gt;\n    arrange(desc(composite_index)) |&gt;\n    transmute(\n      CBSA = NAME,\n      Year = year,\n      `Composite Index` = round(composite_index, 1),\n      `Instantaneous Index` = round(inst_index, 1),\n      `Rate-based Index` = round(rate_index, 1)\n    ) |&gt;\n    head(15),\n  caption = paste0(\"Top CBSAs by Composite Housing Growth (\", latest_year, \")\"),\n  options = list(pageLength = 15, autoWidth = TRUE)\n)\n\n\n\n\n\n\nCode\n# Bottom by Composite\nDT::datatable(\n  hg_latest |&gt;\n    arrange(composite_index) |&gt;\n    transmute(\n      CBSA = NAME,\n      Year = year,\n      `Composite Index` = round(composite_index, 1),\n      `Instantaneous Index` = round(inst_index, 1),\n      `Rate-based Index` = round(rate_index, 1)\n    ) |&gt;\n    head(15),\n  caption = paste0(\"Lowest CBSAs by Composite Housing Growth (\", latest_year, \")\"),\n  options = list(pageLength = 15, autoWidth = TRUE)\n)\n\n\n\n\n\n\n5C) (Optional) Rolling / cumulative smoothing for slow-moving construction\n\n\nCode\n# Rolling 5-year averages (optional polish)\nhousing_growth_roll &lt;- housing_growth |&gt;\n  group_by(GEOID) |&gt;\n  arrange(year, .by_group = TRUE) |&gt;\n  mutate(\n    inst_permits_per_1k_roll5 = RcppRoll::roll_mean(inst_permits_per_1k, n = 5, fill = NA, align = \"right\"),\n    rate_permits_per_1k_growth_roll5 = RcppRoll::roll_mean(rate_permits_per_1k_growth, n = 5, fill = NA, align = \"right\"),\n    composite_index_roll5 = RcppRoll::roll_mean(composite_index, n = 5, fill = NA, align = \"right\")\n  ) |&gt;\n  ungroup()\n\n\nInstantaneous Housing Growth measures how much new supply is being permitted relative to the current population in a given year (permits per 1,000 residents). Rate-based Housing Growth measures how much permitting is happening relative to recent five-year population growth (permits per 1,000 of 5-year population gains). I standardize each into an index with 100 = overall average, which makes them comparable across CBSAs and years. A composite index (equal weights) balances both the current intensity of building and whether permitting keeps up with in-migration. Tables report the top/bottom CBSAs on each metric in the latest year. (Optional: I also compute rolling 5-year averages to smooth year-to-year noise given multi-year construction cycles.)"
  },
  {
    "objectID": "mp02.html#task-6",
    "href": "mp02.html#task-6",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "9.2 TASK 6",
    "text": "9.2 TASK 6\nüß© Step 1: Merge Metrics\n\n\nCode\n# Merge rent burden (rent_burden) and housing growth (housing_growth)\nyimby_df &lt;- rent_burden |&gt;\n  select(GEOID, NAME, year, rent_burden_index, rti) |&gt;\n  left_join(\n    housing_growth |&gt; select(GEOID, year, inst_index, rate_index, composite_index, population),\n    by = c(\"GEOID\", \"year\")\n  ) |&gt;\n  mutate(\n    avg_growth_index = (inst_index + rate_index) / 2,\n    net_affordability_change = rent_burden_index - dplyr::lag(rent_burden_index, 5)\n  )\n\n\nüìä Plot 1 ‚Äî Rent Burden vs Housing Growth\n\n\nCode\nggplot(yimby_df, aes(x = avg_growth_index, y = rent_burden_index, color = year)) +\n  geom_point(alpha = 0.6, size = 2.2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#D55E00\", linewidth = 1) +\n  scale_color_viridis_c(option = \"plasma\", direction = -1) +\n  labs(\n    title = \"Relationship Between Housing Growth and Rent Burden (2009‚Äì2023)\",\n    subtitle = \"Each point is a CBSA; colors indicate year progression\",\n    x = \"Average Housing Growth Index (Instantaneous + Rate-Based)\",\n    y = \"Rent Burden Index (100 = 2009 National Average)\",\n    color = \"Year\",\n    caption = \"Source: U.S. Census Bureau ACS & BLS QCEW\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(plot.title = element_text(face = \"bold\"),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nüß† Interpretation: CBSAs with higher housing growth generally have lower rent burden, supporting the theory that active construction improves affordability. Points trending down-right (high growth, low burden) represent potential ‚ÄúYIMBY success‚Äù metros.\nüìà Plot 2 ‚Äî Evolution of Rent Burden vs Population Growth\n\n\nCode\nggplot(yimby_df, aes(x = population, y = rent_burden_index, group = NAME)) +\n  geom_path(alpha = 0.25, color = \"gray70\") +\n  geom_point(data = subset(yimby_df, NAME %in% c(\"New York-Newark-Jersey City, NY-NJ-PA Metro Area\",\n                                                 \"Los Angeles-Long Beach-Anaheim, CA Metro Area\",\n                                                 \"Houston-The Woodlands-Sugar Land, TX Metro Area\")),\n             aes(color = NAME), size = 3) +\n  scale_color_manual(values = c(\"#E41A1C\", \"#377EB8\", \"#4DAF4A\")) +\n  labs(\n    title = \"Rent Burden vs Population Over Time (Selected CBSAs)\",\n    subtitle = \"Highlighted metros show differing affordability and growth trajectories\",\n    x = \"Total Population\",\n    y = \"Rent Burden Index\",\n    color = \"Highlighted CBSA\",\n    caption = \"2009‚Äì2023, ACS 1-Year Estimates\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(plot.title = element_text(face = \"bold\"),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nüß† Interpretation: New York City: High burden but stable population; modest housing response. Los Angeles: Sustained high burden with slower population growth. Houston: Strong population and housing growth ‚Üí decreasing rent burden ‚Äî a textbook YIMBY success.\nü™∂Highlight ‚ÄúYIMBY‚Äù Champions\n\n\nCode\nyimby_summary &lt;- yimby_df |&gt;\n  group_by(NAME) |&gt;\n  summarise(\n    rent_burden_change = last(rent_burden_index) - first(rent_burden_index),\n    avg_housing_growth = mean(avg_growth_index, na.rm = TRUE),\n    population_change = last(population) - first(population)\n  ) |&gt;\n  mutate(\n    yimby_score = scale(-rent_burden_change) + scale(avg_housing_growth) + scale(population_change)\n  ) |&gt;\n  arrange(desc(yimby_score))\n\nDT::datatable(\n  yimby_summary |&gt; head(15),\n  caption = \"Top 15 YIMBY-Like CBSAs (High Housing Growth, Rising Pop, Falling Rent Burden)\",\n  options = list(pageLength = 15, autoWidth = TRUE)\n)\n\n\n\n\n\n\n\n\nCode\n# Bottom 15 YIMBY score metros (worst performers)\nDT::datatable(\n  yimby_summary |&gt; arrange(yimby_score) |&gt; head(15),\n  caption = \"Bottom 15 CBSAs on YIMBY Score (Low Growth and/or Rising Burden)\",\n  options = list(pageLength = 15, autoWidth = TRUE)\n)\n\n\n\n\n\n\n\n\nCode\n# Extract bottom 3 metros\nbottom3 &lt;- yimby_summary |&gt; arrange(yimby_score) |&gt; slice(1:3)\n\nbottom3\n\n\n\n  \n\n\n\nTo explore YIMBY outcomes, I merged my Rent Burden Index with the Housing Growth Index. The scatterplot shows a negative relationship: metros with higher housing growth tend to have lower rent burdens. The trajectory plot highlights metropolitan examples ‚Äî New York, Los Angeles, and Houston ‚Äî revealing distinct patterns. Houston demonstrates population and construction growth accompanied by improving affordability, identifying it as a strong YIMBY case. The tabular summary quantifies this behavior, ranking metros by a composite ‚ÄúYIMBY Score‚Äù that rewards rising population and housing supply alongside declining rent burden."
  },
  {
    "objectID": "mp02.html#task-7-policy-brief-federal-yimby-incentive-program",
    "href": "mp02.html#task-7-policy-brief-federal-yimby-incentive-program",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "9.3 Task 7: Policy Brief ‚Äî Federal YIMBY Incentive Program",
    "text": "9.3 Task 7: Policy Brief ‚Äî Federal YIMBY Incentive Program\nBackground\nAcross the United States, rising housing costs have outpaced income growth, leaving many households rent-burdened and forcing workers to live farther from job centers. Our analysis of ACS and BLS data from 2009‚Äì2023 shows a consistent pattern: metros that build more housing experience lower rent burdens and stronger population growth.\nThese outcomes support a local policy approach aligned with the ‚ÄúYes In My Backyard‚Äù (YIMBY) movement, which promotes increased housing supply through zoning reform, streamlined permitting, and support for infill and multifamily development.\nEvidence from the Data\nOur combined Rent Burden Index and Housing Growth Index reveal meaningful differences in affordability between metros that build housing and those that restrict construction. YIMBY Example ‚Äî Dallas‚ÄìFort Worth‚ÄìArlington, TX Strong housing permitting activity Large population inflow Rent burden stabilized while the metro expanded rapidly NIMBY-Constrained Example ‚Äî Springfield, MA Higher rent burden over time Limited growth in residential development Workers face affordability challenges and limited housing options These contrasting outcomes show that housing supply responsiveness plays a key role in affordability and household stability.\nProposed Congressional Sponsors\nRole Metro Rationale Primary Sponsor Dallas‚ÄìFort Worth‚ÄìArlington, Texas Demonstrated ability to add housing while supporting population and job growth Co-Sponsor Springfield, Massachusetts High rent burden and limited supply growth; stands to benefit most from YIMBY incentives This pairing highlights both success and needs-based urgency.\nKey Support from Labor & Industry\nTo build political momentum, we identify two occupations with meaningful employment in both metros that stand to benefit from improved housing affordability: Occupation Group Why Support the Policy Teachers and Education Workers Lower housing costs improve educator retention and support school systems Healthcare Workers (nurses, medical technicians) Affordable housing near hospitals improves staffing stability and access to care Affordable housing allows these essential workers to live in the communities they serve, strengthening local labor markets and public services.\nMetrics to Evaluate Eligibility\nWe propose using simple, transparent metrics supported by federal public data: Metric Explanation\nRent Burden Index Tracks how much of household income goes to rent (lower = more affordable)\nHousing Growth Index Measures new permits per capita and relative to population growth (higher = more building)\nThese indicators help identify metros that are improving affordability and those that need support.\nPolicy Recommendation\nCongress should establish a Federal YIMBY Incentive Program that: Provides competitive grants to local governments reducing rent burden Rewards metros that increase housing production relative to population Supports zoning modernization and streamlined permitting in constrained metros Encourages infrastructure investment tied to smart growth and density The principle is straightforward: communities that build housing to meet demand should receive federal support.\nConclusion\nHousing affordability is essential to economic mobility, community stability, and workforce strength. The contrast between Dallas and Springfield demonstrates how supply-responsive policy can improve affordability outcomes. A federal YIMBY incentive program would help cities modernize housing policy, support essential workers, and ensure Americans can live near opportunity. More homes mean stronger families, healthier labor markets, and a more prosperous nation."
  },
  {
    "objectID": "mp02.html#multi-table-questions",
    "href": "mp02.html#multi-table-questions",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "Multi-Table Questions",
    "text": "Multi-Table Questions\n\nQ1.Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nCode\nPERMITS |&gt;\n  filter(year &gt;= 2010, year &lt;= 2019) |&gt;\n  left_join(INCOME |&gt; select(GEOID, NAME) |&gt; distinct(),\n            by = c(\"CBSA\" = \"GEOID\")) |&gt;\n  group_by(NAME) |&gt;\n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt;\n  arrange(desc(total_units)) |&gt;\n  slice(1)\n\n\n\n  \n\n\n\nBetween 2010 and 2019, the Houston‚ÄìPasadena‚ÄìThe Woodlands, TX Metropolitan Area permitted approximately 482,075 new housing units, the highest of any CBSA in the United States during that decade.\n\n\n2.In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\nPERMITS |&gt;\n  filter(CBSA == 10740) |&gt;\n  arrange(desc(new_housing_units_permitted)) |&gt;\n  slice(1)\n\n\n\n  \n\n\n\nThe Albuquerque, NM Metropolitan Area (CBSA 10740) permitted the most new housing units in 2021, with approximately 4,021 new units. Although 2021 shows the highest recorded number, this may partly reflect Covid-19-related reporting anomalies ‚Äî for instance, delayed or batched permit filings after the 2020 survey pause.\n\n\n3.Which state (not CBSA) had the highest average individual income in 2015?\n\n\nCode\n# Step 1: Filter to year 2015\nincome_2015 &lt;- INCOME |&gt; filter(year == 2015)\nhouseholds_2015 &lt;- HOUSEHOLDS |&gt; filter(year == 2015)\npopulation_2015 &lt;- POPULATION |&gt; filter(year == 2015)\n\n# Step 2: Join all three by GEOID\nincome_state &lt;- income_2015 |&gt;\n  left_join(households_2015 |&gt; select(GEOID, households),\n             by = \"GEOID\") |&gt;\n  left_join(population_2015 |&gt; select(GEOID, population),\n             by = \"GEOID\")\n\n# Step 3: Compute total income per CBSA\nincome_state &lt;- income_state |&gt;\n  mutate(total_income = household_income * households)\n\n# Step 4: Extract state abbreviation from NAME\nincome_state &lt;- income_state |&gt;\n  mutate(state = str_extract(NAME, \", (.{2})\"))\n\n# Step 5: Create a reference table of state names\nstate_df &lt;- data.frame(\n  abb  = c(state.abb, \"DC\", \"PR\"),\n  name = c(state.name, \"District of Columbia\", \"Puerto Rico\")\n)\n\n# Step 6: Compute total and average income per state\nstate_income_summary &lt;- income_state |&gt;\n  group_by(state) |&gt;\n  summarise(\n    total_income = sum(total_income, na.rm = TRUE),\n    total_population = sum(population, na.rm = TRUE),\n    avg_individual_income = total_income / total_population\n  ) |&gt;\n  left_join(state_df, by = c(\"state\" = \"abb\")) |&gt;\n  arrange(desc(avg_individual_income))\n\n# Step 7: Display the top state\nstate_income_summary |&gt; slice(1)\n\n\n\n  \n\n\n\nIn 2015, the District of Columbia (DC) had the highest average individual income among all U.S. states, with an estimated $33,233 per person.\n\n\n4.What is the last year in which the NYC CBSA had the most data scientists in the country?\n\n\nCode\n# Step 1: Filter BLS data for data scientists and business analysts\n# NAICS code 5182 = Data processing, hosting, and related services\ndata_sci &lt;- WAGES |&gt;\n  filter(INDUSTRY == 5182)\n\n# Step 2: Identify which CBSA had the most data scientists each year\nmost_ds_by_year &lt;- data_sci |&gt;\n  group_by(YEAR, FIPS) |&gt;\n  summarise(total_employment = sum(EMPLOYMENT, na.rm = TRUE)) |&gt;\n  arrange(YEAR, desc(total_employment)) |&gt;\n  slice_head(n = 1) |&gt;\n  ungroup()\n\n# Step 3: Prepare CBSA mapping for readability (from INCOME table)\ncbsa_names &lt;- INCOME |&gt; \n  select(GEOID, NAME) |&gt; \n  distinct() |&gt; \n  mutate(std_cbsa = paste0(\"C\", GEOID))\n\n# Step 4: Format CBSA codes in both datasets for joining\nmost_ds_by_year &lt;- most_ds_by_year |&gt;\n  mutate(std_cbsa = paste0(FIPS, \"0\")) |&gt;\n  left_join(cbsa_names, by = \"std_cbsa\")\n\n# Step 5: View which city led each year\nmost_ds_by_year |&gt; select(YEAR, NAME, total_employment) |&gt; arrange(YEAR)\n\n\n\n  \n\n\n\nThe last year in which the New York‚ÄìNewark‚ÄìJersey City, NY‚ÄìNJ‚ÄìPA CBSA had the most data scientists in the U.S. was 2015.\n\n\n5.What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\n\nCode\nnyc_finance_share &lt;- WAGES |&gt;\n  filter(str_detect(FIPS, \"^C3562\")) |&gt;   # match any NYC-related CBSA starting with C3562\n  group_by(YEAR) |&gt;\n  summarise(\n    finance_wages = sum(ifelse(INDUSTRY == 52, TOTAL_WAGES, 0), na.rm = TRUE),\n    total_wages   = sum(TOTAL_WAGES, na.rm = TRUE),\n    frac_finance  = finance_wages / total_wages\n  ) |&gt;\n  arrange(desc(frac_finance))\n\nnyc_finance_share |&gt; slice(1)\n\n\n\n  \n\n\n\nPeak Year: 2014 Fraction of Total Wages (Finance & Insurance): ‚âà 4.6%"
  },
  {
    "objectID": "mp02.html#task-4-building-indices-of-housing-affordability-and-housing-stock-growth",
    "href": "mp02.html#task-4-building-indices-of-housing-affordability-and-housing-stock-growth",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "TASK 4: Building Indices of Housing Affordability and Housing Stock Growth",
    "text": "TASK 4: Building Indices of Housing Affordability and Housing Stock Growth\n\n\nCode\n# Merge INCOME and RENT data and calculate rent-to-income ratio\nrent_income &lt;- INCOME %&gt;%\n  select(GEOID, NAME, year, household_income) %&gt;%\n  inner_join(RENT %&gt;% select(GEOID, year, monthly_rent), by = c(\"GEOID\", \"year\")) %&gt;%\n  mutate(rent_to_income = (monthly_rent * 12) / household_income)  # annual rent share of income\n\n# View the first few rows to verify\nhead(rent_income)\n\n\n\n  \n\n\n\n\n\nCode\n# Standardize Rent Burden - rescale between 0 to 100\nrent_income &lt;- rent_income %&gt;%\n  mutate(rent_burden_index = scales::rescale(rent_to_income, to = c(0, 100)))\n\n# Verify the rent burden index calculation\nhead(rent_income)\n\n\n\n  \n\n\n\n\n\nCode\n# Standardize Rent Burden using Z-score\nbaseline_mean &lt;- mean(rent_income$rent_to_income, na.rm = TRUE)\nrent_burden_sd &lt;- sd(rent_income$rent_to_income, na.rm = TRUE)\n\nrent_income &lt;- rent_income %&gt;%\n  mutate(rent_burden_index_std = ((rent_to_income - baseline_mean) / rent_burden_sd) * 10 + 50,\n         rent_burden_index_std = pmax(0, pmin(100, rent_burden_index_std)))  # Clip to range [0, 100]\n\n# Verify the standardized rent burden index\nhead(rent_income)\n\n\n\n  \n\n\n\n\n\nCode\n# Summary for Rent Burden: Average and Median by Year\nsummary_tbl &lt;- rent_income %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    avg_burden = mean(rent_burden_index, na.rm = TRUE),\n    median_burden = median(rent_burden_index, na.rm = TRUE)\n  )\n\n# Display summary as an interactive table\nDT::datatable(summary_tbl,\n              caption = \"Average and Median Rent Burden Index by Year\",\n              options = list(pageLength = 10, scrollX = TRUE))\n\n\n\n\n\n\n\n\nCode\n# Filter data for New York City\nmetro_name &lt;- \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\"\nmetro_burden &lt;- rent_income %&gt;%\n  filter(NAME == metro_name) %&gt;%\n  arrange(year)\n\n# Plot Rent Burden Over Time for New York City\nggplot(metro_burden, aes(x = year, y = rent_burden_index)) +\n  geom_line(color = \"#0072B2\", linewidth = 1) +\n  geom_point(color = \"#0072B2\") +\n  labs(\n    title = paste(\"Rent Burden Over Time -\", metro_name),\n    x = \"Year\",\n    y = \"Rent Burden Index (0 = Least, 100 = Most Burdened)\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Get the latest year\nlatest_year &lt;- max(rent_income$year, na.rm = TRUE)\n\n# Find CBSAs with highest and lowest rent burden in the latest year\nrent_extremes &lt;- rent_income %&gt;%\n  filter(year == latest_year) %&gt;%\n  arrange(desc(rent_burden_index)) %&gt;%\n  mutate(rank = row_number())\n\n# Top 10 most rent-burdened CBSAs\ntop10_highest &lt;- rent_extremes %&gt;% slice_head(n = 10)\n\n# Top 10 least rent-burdened CBSAs\ntop10_lowest  &lt;- rent_extremes %&gt;% slice_tail(n = 10)\n\n# Display Top 10 Most Rent-Burdened CBSAs in an interactive table\nDT::datatable(top10_highest,\n              caption = paste(\"Top 10 Most Rent-Burdened CBSAs (\", latest_year, \")\", sep = \"\"),\n              options = list(pageLength = 10, scrollX = TRUE))\n\n\n\n\n\n\nCode\n# Display Top 10 Least Rent-Burdened CBSAs in an interactive table\nDT::datatable(top10_lowest,\n              caption = paste(\"Top 10 Least Rent-Burdened CBSAs (\", latest_year, \")\", sep = \"\"),\n              options = list(pageLength = 10, scrollX = TRUE))"
  },
  {
    "objectID": "mp02.html#task-5-housing-growth",
    "href": "mp02.html#task-5-housing-growth",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "TASK 5: Housing Growth",
    "text": "TASK 5: Housing Growth\n\n\nCode\n# Step 1: Merge Population and Permits data\npop_for_join &lt;- POPULATION |&gt;\n  mutate(cbsa_int = as.integer(GEOID)) |&gt;\n  select(cbsa_int, GEOID, NAME, year, population)\n\npermits_clean &lt;- PERMITS |&gt;\n  rename(cbsa_int = CBSA, permits = new_housing_units_permitted) |&gt;\n  select(cbsa_int, year, permits)\n\nhousing_base &lt;- pop_for_join |&gt;\n  left_join(permits_clean, by = c(\"cbsa_int\", \"year\"))\n\n# Step 2: Calculate 5-year population growth within each CBSA (starting from 2014)\nhousing_base &lt;- housing_base |&gt;\n  group_by(GEOID) |&gt;\n  arrange(year, .by_group = TRUE) |&gt;\n  mutate(\n    population_5y_ago = dplyr::lag(population, 5),\n    pop_growth_5y = population - population_5y_ago\n  ) |&gt;\n  ungroup()\n\n# Step 3: Instantaneous Housing Growth (permits per 1,000 residents)\nhousing_base &lt;- housing_base |&gt;\n  mutate(inst_permits_per_1k = 1000 * permits / population)\n\n# Step 4: Rate-Based Housing Growth (permits per 1,000 residents gained in past 5 years)\nhousing_base &lt;- housing_base |&gt;\n  mutate(\n    rate_permits_per_1k_growth = dplyr::if_else(\n      !is.na(pop_growth_5y) & pop_growth_5y &gt; 0,\n      1000 * permits / pop_growth_5y,\n      NA_real_\n    )\n  )\n\n# Step 5: Standardize both metrics to a 0-100 scale\ninst_baseline &lt;- mean(housing_base$inst_permits_per_1k, na.rm = TRUE)\nrate_baseline &lt;- mean(housing_base$rate_permits_per_1k_growth, na.rm = TRUE)\n\nhousing_growth &lt;- housing_base |&gt;\n  mutate(\n    inst_index = 100 * inst_permits_per_1k / inst_baseline,\n    rate_index = 100 * rate_permits_per_1k_growth / rate_baseline,\n    composite_index = (inst_index + rate_index) / 2  # Equal weights for composite index\n  )\n\n\nTop 15 CBSAs by Instantaneous Housing Growth (2023)\n\n\nCode\n# Step 6: Create table for top 15 CBSAs by Instantaneous Housing Growth in the latest year\nlatest_year &lt;- max(housing_growth$year, na.rm = TRUE)\n\nhg_latest &lt;- housing_growth |&gt;\n  filter(year == latest_year) |&gt;\n  distinct(GEOID, NAME, year,\n           inst_permits_per_1k, rate_permits_per_1k_growth,\n           inst_index, rate_index, composite_index)\n\n# Top by Instantaneous\nDT::datatable(\n  hg_latest |&gt;\n    arrange(desc(inst_index)) |&gt;\n    transmute(\n      CBSA = NAME,\n      Year = year,\n      `Permits per 1k Residents` = round(inst_permits_per_1k, 2),\n      `Instantaneous Index` = round(inst_index, 1)\n    ) |&gt;\n    head(15),\n  caption = paste0(\"Top 15 CBSAs by Instantaneous Housing Growth (\", latest_year, \")\"),\n  options = list(pageLength = 15, autoWidth = TRUE)\n)\n\n\n\n\n\n\nTop 15 CBSAs by Rate-Based Housing Growth (2023)\n\n\nCode\n# Top by Rate-based Housing Growth\nDT::datatable(\n  hg_latest |&gt;\n    arrange(desc(rate_index)) |&gt;\n    transmute(\n      CBSA = NAME,\n      Year = year,\n      `Permits per 1k of 5y Pop Growth` = round(rate_permits_per_1k_growth, 2),\n      `Rate-based Index` = round(rate_index, 1)\n    ) |&gt;\n    head(15),\n  caption = paste0(\"Top 15 CBSAs by Rate-based Housing Growth (\", latest_year, \")\"),\n  options = list(pageLength = 15, autoWidth = TRUE)\n)\n\n\n\n\n\n\nTop 15 CBSAs by Composite Housing Growth (2023)\n\n\nCode\n# Top by Composite Housing Growth\nDT::datatable(\n  hg_latest |&gt;\n    arrange(desc(composite_index)) |&gt;\n    transmute(\n      CBSA = NAME,\n      Year = year,\n      `Composite Index` = round(composite_index, 1),\n      `Instantaneous Index` = round(inst_index, 1),\n      `Rate-based Index` = round(rate_index, 1)\n    ) |&gt;\n    head(15),\n  caption = paste0(\"Top 15 CBSAs by Composite Housing Growth (\", latest_year, \")\"),\n  options = list(pageLength = 15, autoWidth = TRUE)\n)\n\n\n\n\n\n\nBottom 15 CBSAs by Composite Housing Growth (2023)\n\n\nCode\n# Bottom by Composite Housing Growth\nDT::datatable(\n  hg_latest |&gt;\n    arrange(composite_index) |&gt;\n    transmute(\n      CBSA = NAME,\n      Year = year,\n      `Composite Index` = round(composite_index, 1),\n      `Instantaneous Index` = round(inst_index, 1),\n      `Rate-based Index` = round(rate_index, 1)\n    ) |&gt;\n    head(15),\n  caption = paste0(\"Bottom 15 CBSAs by Composite Housing Growth (\", latest_year, \")\"),\n  options = list(pageLength = 15, autoWidth = TRUE)\n)\n\n\n\n\n\n\nInterpretation of Results: Instantaneous Housing Growth measures the current level of housing supply relative to the population in each year.\nRate-Based Housing Growth compares how much housing supply has been built relative to recent population growth.\nComposite Index combines the two metrics, providing a holistic view of housing growth in each CBSA.\nThe top CBSAs will be those that have built the most housing relative to their population size and growth, while the bottom CBSAs will be those struggling with slower housing development despite population increases."
  },
  {
    "objectID": "mp02.html#supporting-the-yimby-movement-for-affordable-housing",
    "href": "mp02.html#supporting-the-yimby-movement-for-affordable-housing",
    "title": "Mini-Project #02 ‚Äî Making Backyards Affordable for All",
    "section": "Supporting the YIMBY Movement for Affordable Housing",
    "text": "Supporting the YIMBY Movement for Affordable Housing\n\nIntroduction\nAs a political lobbyist for the national Yes In My Backyard (YIMBY) organization, I am urging Congress to establish a federal program that incentivizes local municipalities to adopt YIMBY policies. These policies focus on expanding housing supply through zoning reform, relaxed permitting processes, and support for multifamily and high-density housing development.\n\n\nProposed Congressional Sponsors\nTo drive this legislation forward, we need the support of local congressional representatives. I propose the following pair of sponsors:\n\nPrimary Sponsor: Representative from Austin-Round Rock-Georgetown, TX\n\nRationale: Austin is a prime example of a YIMBY success. Over the past decade, the city has dramatically increased its housing supply, resulting in a decrease in rent burden despite significant population growth. Austin‚Äôs housing growth metrics and rent burden index show a consistent trend of improvement, demonstrating that more housing can help maintain affordability.\n\nCo-Sponsor: Representative from New York-Newark-Jersey City, NY-NJ-PA\n\nRationale: New York City, despite its economic vibrancy, has faced an affordability crisis due to slow housing growth relative to population increase. Rent burden in New York continues to rise, and the lack of sufficient housing supply exacerbates the affordability gap. The city‚Äôs struggle represents the need for policy change, making it a crucial voice for this bill.\n\n\n\n\nSupporting Labor & Industry Groups\nTo gather support for this bill, we must appeal to labor unions and industry trade organizations. Two key occupations that would benefit from this bill are:\n\nTeachers and Education Workers:\n\nImpact: Teachers and other education workers often face high rent burdens, particularly in cities with tight housing markets. By promoting more housing construction, this bill would alleviate rent pressures, making it easier for teachers to afford living near schools, thus improving teacher retention and reducing turnover.\nBenefit: Teachers would benefit directly from lower rent and increased housing supply, making it more feasible for them to live in the same communities where they work. Additionally, increased affordability would free up more disposable income, contributing to overall economic stability.\n\nHealthcare Workers (Nurses, Technicians):\n\nImpact: Healthcare professionals are often underpaid relative to the high cost of living in metropolitan areas. High housing costs contribute to burnout, low morale, and difficulties in staffing. By expanding the housing supply, this bill would help keep housing costs in check, enabling healthcare workers to afford homes near their workplaces.\nBenefit: Lower housing costs would directly increase healthcare workers‚Äô disposable income and improve job satisfaction, reducing turnover rates and increasing the quality of care. This would also improve continuity of care by enabling healthcare workers to live within commuting distance of their hospitals or clinics.\n\n\n\n\nMetrics for Identifying ‚ÄúGood‚Äù (High-YIMBY) Cities\nTo ensure that the program effectively identifies cities that can benefit from federal support, the following metrics should be used:\n\nRent Burden Index (RBI):\n\nDefinition: The Rent Burden Index measures the share of income that households spend on rent. It is calculated as the rent-to-income ratio, where higher values indicate higher rent burden.\nStandardization: We standardize this index to a baseline of 100, where 100 represents the national average in 2009. Cities with values above 100 are considered more burdened, while values below 100 indicate lower rent burdens.\n\nHousing Growth Index (HGI):\n\nDefinition: The Housing Growth Index compares the number of housing units permitted in a given year relative to the current population of the city. This index also considers the housing units permitted relative to population growth over a five-year period, helping to capture both short-term and long-term growth patterns.\nStandardization: The Housing Growth Index is normalized to a baseline of 100, representing the average growth across all CBSAs.\n\nMillennial Appeal Index (MAI):\n\nDefinition: This metric gauges the proportion of employment in arts and entertainment, which is typically driven by younger adults, such as Millennials.\nBenefit: High levels of arts and entertainment employment are associated with cultural vibrancy and young adult attraction, making cities with high MAI more likely to benefit from housing policies that support young professionals.\n\n\n\n\nPolicy Recommendations\nCongress should establish a Federal YIMBY Incentive Program with the following objectives:\n\nProvide Competitive Grants: Offer funding to cities that have shown a decrease in rent burden and a commitment to housing development. These cities would be eligible for grants to support zoning modernization, streamlined permitting, and multifamily housing construction.\nSupport Housing Expansion: Encourage cities to expand housing development to meet growing demand, particularly in areas with high rent burdens and limited housing stock.\nCreate Infrastructure Investment: Support infrastructure development tied to housing growth, including improvements to transportation, utilities, and community services, to accommodate new residents and reduce the strain on existing systems.\nPromote Smart Growth: Focus on sustainable development that includes both housing and necessary infrastructure, with an emphasis on transit-oriented development and environmentally-friendly building practices.\n\n\n\nConclusion\nThis bill represents a proactive and bipartisan approach to addressing the housing affordability crisis across the United States. By targeting cities with high rent burdens and promoting housing development in growing metros, we can create more affordable, vibrant, and sustainable cities for American workers and their families. The contrast between YIMBY success in Austin and the struggles of New York City demonstrates the need for federal support to incentivize housing construction, reduce rent burdens, and improve economic opportunities for all.\nThrough this bill, we will empower local governments to act on housing policy, support essential workers, and ultimately provide affordable housing for more Americans. A strong commitment to building more homes will benefit both individuals and communities, ensuring greater economic mobility and stability for all."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "",
    "text": "Code\nlibrary(sf)\nlibrary(dplyr)\nlibrary(httr2)\nlibrary(purrr)\nlibrary(glue)\nlibrary(ggplot2)\nlibrary(knitr)\n\ndir.create(\"data/mp03\", recursive = TRUE, showWarnings = FALSE)\nIntroduction\nNew York City‚Äôs Parks and Recreation Department manages nearly 900,000 trees across five boroughs. This project explores the NYC Tree Map dataset and the City Council District boundaries to understand how trees are distributed throughout the city and identify opportunities for improving canopy equity.\nThe analysis will:"
  },
  {
    "objectID": "mp03.html#council-district-boundaries",
    "href": "mp03.html#council-district-boundaries",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "1.1 Council District Boundaries",
    "text": "1.1 Council District Boundaries\nWe begin by reading the shapefile containing all 51 NYC Council Districts and converting it to the WGS 84 coordinate system for compatibility with other datasets.\n\n\nCode\ndownload_nyc_council &lt;- function() {\n  dir.create(\"data/mp03\", showWarnings = FALSE, recursive = TRUE)\n\n  zip_path &lt;- \"data/mp03/nycc_25c.zip\"\n  unzip_dir &lt;- \"data/mp03/nycc_25c\"\n\n  # 1. Download if needed\n  if (!file.exists(zip_path)) {\n    url &lt;- \"https://www.nyc.gov/assets/planning/download/zip/data-maps/open-data/nycc_25c.zip\"\n    download.file(url, zip_path, mode = \"wb\")\n  }\n\n  # 2. Unzip if needed\n  if (!dir.exists(unzip_dir)) {\n    unzip(zip_path, exdir = unzip_dir)\n  }\n\n  # 3. Find the actual .shp file (your case: nycc_25c/nycc_25c/nycc.shp)\n  shp_file &lt;- list.files(unzip_dir, pattern = \"\\\\.shp$\", recursive = TRUE, full.names = TRUE)[1]\n\n  if (is.na(shp_file)) stop(\"Shapefile not found. Check folder structure.\")\n\n  # 4. Read with sf\n  nyc &lt;- sf::st_read(shp_file, quiet = TRUE)\n\n  # 5. Transform to WGS84\n  nyc &lt;- sf::st_transform(nyc, crs = \"WGS84\")\n\n  return(nyc)\n}\n\n# Run it\ndistricts &lt;- download_nyc_council()\ndistricts"
  },
  {
    "objectID": "mp03.html#visualize-council-districts",
    "href": "mp03.html#visualize-council-districts",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "1.2 Visualize Council Districts",
    "text": "1.2 Visualize Council Districts\n\n\nCode\nggplot(districts) +\n  geom_sf(data = districts,fill = \"lightgreen\", color = \"grey40\", linewidth = 0.4) +\n  labs(\n    title = \"NYC City Council Districts\",\n    subtitle = \"51 polygons across five boroughs\",\n    caption = \"Source: NYC Department of City Planning\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.major = element_line(linewidth = 0.2, color = \"grey90\"),\n    plot.title = element_text(face = \"bold\")\n  )"
  },
  {
    "objectID": "mp03.html#setup-for-api-download",
    "href": "mp03.html#setup-for-api-download",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "2.1 Setup for API download",
    "text": "2.1 Setup for API download\n\n\nCode\nlibrary(httr2)\nlibrary(purrr)\nlibrary(sf)\n\nsoc_base &lt;- \"https://data.cityofnewyork.us/resource/hn5i-inap.geojson\"\n\nfetch_tree_pages &lt;- function(limit = 50000, max_pages = 25, dest_dir = \"data/mp03\") {\n  dir.create(dest_dir, recursive = TRUE, showWarnings = FALSE)\n  files &lt;- c()\n  i &lt;- 0L\n\n  repeat {\n    offset &lt;- i * limit\n    out_file &lt;- file.path(dest_dir, sprintf(\"trees_page_%03d.geojson\", i))\n\n    if (!file.exists(out_file)) {\n\n      req &lt;- request(soc_base) |&gt;\n        req_url_query(\n          `$limit` = limit,\n          `$offset` = offset,\n          `$select` = \"*\"        # ‚Üê REQUIRED !!!\n        ) |&gt;\n        req_user_agent(\"STA9750-mp03\") |&gt;\n        req_timeout(120)\n\n      resp &lt;- req_perform(req)\n      raw &lt;- resp_body_raw(resp)\n      if (length(raw) &lt; 1000L) break\n      writeBin(raw, out_file)\n    }\n\n    files &lt;- c(files, out_file)\n\n    g &lt;- tryCatch(st_read(out_file, quiet = TRUE), error = function(e) NULL)\n    n &lt;- if (!is.null(g)) nrow(g) else limit\n\n    if (n &lt; limit) break\n    i &lt;- i + 1L\n    if (i &gt;= max_pages) break\n  }\n\n  files\n}"
  },
  {
    "objectID": "mp03.html#download-a-small-test-sample-for-speed",
    "href": "mp03.html#download-a-small-test-sample-for-speed",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "2.2 Download a small test sample (for speed)",
    "text": "2.2 Download a small test sample (for speed)\n\n\nCode\ntree_files &lt;- fetch_tree_pages(limit = 50000, max_pages = 25)\ntree_files\n\n\n[1] \"data/mp03/trees_page_000.geojson\""
  },
  {
    "objectID": "mp03.html#read-and-combine-all-tree-data",
    "href": "mp03.html#read-and-combine-all-tree-data",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "2.3 Read and combine all tree data",
    "text": "2.3 Read and combine all tree data\n\n\nCode\nread_trees &lt;- function(files) {\npurrr::map(files, ~ suppressWarnings(st_read(.x, quiet = TRUE))) |&gt;\nlist_rbind()\n}\n\ntrees_sf &lt;- read_trees(tree_files)\n\nif (is.na(st_crs(trees_sf))) {\ntrees_sf &lt;- st_set_crs(trees_sf, \"WGS84\")\n}\n\n# Check full data size\n\nnrow(trees_sf)\n\n\n[1] 20000"
  },
  {
    "objectID": "mp03.html#spatial-join-trees-districts",
    "href": "mp03.html#spatial-join-trees-districts",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "üß©Spatial Join (trees + districts)",
    "text": "üß©Spatial Join (trees + districts)\nWe‚Äôll use st_join() to attach each tree to the district polygon it lies in.\n\n\nCode\n# Join tree points to council districts\n\ntrees_joined &lt;- st_join(trees_sf, districts, join = st_intersects)\ntrees_joined\n\n\n\n\nCode\n# Task 4 summary table\n\nby_dist &lt;- trees_joined %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(CounDist) %&gt;%\n  summarise(\n    total_trees = n(),\n    dead_trees = sum(tpcondition == \"Dead\", na.rm = TRUE),\n    Shape_Area = mean(Shape_Area, na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    tree_density = total_trees / Shape_Area,\n    dead_frac = dead_trees / total_trees\n  )"
  },
  {
    "objectID": "mp03.html#compute-tree-counts-density-and-dead-trees",
    "href": "mp03.html#compute-tree-counts-density-and-dead-trees",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "üßÆ 4.2 Compute Tree Counts, Density, and Dead Trees",
    "text": "üßÆ 4.2 Compute Tree Counts, Density, and Dead Trees\n\n\nCode\nlibrary(dplyr)\n\n# Calculate per-district statistics\n\nby_dist &lt;- trees_joined |&gt;\nst_drop_geometry() |&gt;                     # Drop geometry to work as normal table\ngroup_by(CounDist) |&gt;\nsummarize(\ntotal_trees = n(),\ndead_trees = sum(tolower(tpcondition) %in% c(\"dead\", \"stump\"), na.rm = TRUE),\n.groups = \"drop\"\n) |&gt;\nleft_join(st_drop_geometry(districts) |&gt; select(CounDist, Shape_Area), by = \"CounDist\") |&gt;\nmutate(\ntree_density = total_trees / Shape_Area,\ndead_frac = dead_trees / total_trees\n)\n\nby_dist |&gt;\n  select(CounDist, total_trees, dead_trees, Shape_Area, tree_density, dead_frac) |&gt;\n  arrange(CounDist) |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounDist\ntotal_trees\ndead_trees\nShape_Area\ntree_density\ndead_frac\n\n\n\n\n1\n173\n21\n78106503\n2.2e-06\n0.1213873\n\n\n2\n232\n40\n48322121\n4.8e-06\n0.1724138\n\n\n3\n319\n48\n76315832\n4.2e-06\n0.1504702\n\n\n4\n316\n53\n66802515\n4.7e-06\n0.1677215\n\n\n5\n189\n22\n37752246\n5.0e-06\n0.1164021\n\n\n6\n198\n34\n82672687\n2.4e-06\n0.1717172\n\n\n7\n183\n36\n55186140\n3.3e-06\n0.1967213\n\n\n8\n138\n40\n106989876\n1.3e-06\n0.2898551\n\n\n9\n183\n31\n56263769\n3.3e-06\n0.1693989\n\n\n10\n131\n24\n76997844\n1.7e-06\n0.1832061\n\n\n11\n235\n104\n216987852\n1.1e-06\n0.4425532\n\n\n12\n177\n48\n131040796\n1.4e-06\n0.2711864\n\n\n13\n433\n161\n328361699\n1.3e-06\n0.3718245\n\n\n14\n64\n22\n52585062\n1.2e-06\n0.3437500\n\n\n15\n78\n37\n102495308\n8.0e-07\n0.4743590\n\n\n16\n80\n48\n62082481\n1.3e-06\n0.6000000\n\n\n17\n132\n48\n115113830\n1.1e-06\n0.3636364\n\n\n18\n150\n64\n110601770\n1.4e-06\n0.4266667\n\n\n19\n756\n207\n334738191\n2.3e-06\n0.2738095\n\n\n20\n273\n105\n144833269\n1.9e-06\n0.3846154\n\n\n21\n125\n42\n130912211\n1.0e-06\n0.3360000\n\n\n22\n217\n77\n150395658\n1.4e-06\n0.3548387\n\n\n23\n630\n205\n311520682\n2.0e-06\n0.3253968\n\n\n24\n353\n126\n186824791\n1.9e-06\n0.3569405\n\n\n25\n165\n44\n63861388\n2.6e-06\n0.2666667\n\n\n26\n195\n72\n168180998\n1.2e-06\n0.3692308\n\n\n27\n405\n122\n210809911\n1.9e-06\n0.3012346\n\n\n28\n361\n86\n175137630\n2.1e-06\n0.2382271\n\n\n29\n328\n109\n127849354\n2.6e-06\n0.3323171\n\n\n30\n505\n150\n168734193\n3.0e-06\n0.2970297\n\n\n31\n359\n138\n507654144\n7.0e-07\n0.3844011\n\n\n32\n506\n186\n358667790\n1.4e-06\n0.3675889\n\n\n33\n242\n70\n110198297\n2.2e-06\n0.2892562\n\n\n34\n296\n67\n105180459\n2.8e-06\n0.2263514\n\n\n35\n494\n80\n79440619\n6.2e-06\n0.1619433\n\n\n36\n205\n46\n76224396\n2.7e-06\n0.2243902\n\n\n37\n129\n35\n101031674\n1.3e-06\n0.2713178\n\n\n38\n408\n89\n151771974\n2.7e-06\n0.2181373\n\n\n39\n489\n129\n118294553\n4.1e-06\n0.2638037\n\n\n40\n216\n32\n73666390\n2.9e-06\n0.1481481\n\n\n41\n312\n43\n79271987\n3.9e-06\n0.1378205\n\n\n42\n189\n29\n201334162\n9.0e-07\n0.1534392\n\n\n43\n208\n39\n75477511\n2.8e-06\n0.1875000\n\n\n44\n349\n62\n99194858\n3.5e-06\n0.1776504\n\n\n45\n355\n64\n117904762\n3.0e-06\n0.1802817\n\n\n46\n556\n53\n277719690\n2.0e-06\n0.0953237\n\n\n47\n583\n82\n147530657\n4.0e-06\n0.1406518\n\n\n48\n586\n101\n109815036\n5.3e-06\n0.1723549\n\n\n49\n852\n230\n330301028\n2.6e-06\n0.2699531\n\n\n50\n3332\n571\n665196534\n5.0e-06\n0.1713685\n\n\n51\n1610\n392\n657989092\n2.4e-06\n0.2434783\n\n\n\n\n\n‚úÖ This gives you a clean summary of:\n\nTotal trees per district: District 50\nDensity (trees per area): District 35\nDead tree fraction: District 16"
  },
  {
    "objectID": "mp03.html#identify-key-districts",
    "href": "mp03.html#identify-key-districts",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "üåü 4.3 Identify Key Districts",
    "text": "üåü 4.3 Identify Key Districts\n\n\nCode\n# Most trees\n\nmost_trees &lt;- by_dist |&gt; arrange(desc(total_trees)) |&gt; slice(1)\n\n# Highest density\n\nhighest_density &lt;- by_dist |&gt; arrange(desc(tree_density)) |&gt; slice(1)\n\n# Highest dead fraction\n\nhighest_dead_frac &lt;- by_dist |&gt; filter(total_trees &gt; 50) |&gt; arrange(desc(dead_frac)) |&gt; slice(1)\n\nkey_districts &lt;- bind_rows(\n  most_trees |&gt; mutate(category = \"Most Trees\"),\n  highest_density |&gt; mutate(category = \"Highest Density\"),\n  highest_dead_frac |&gt; mutate(category = \"Highest Dead Fraction\")\n) |&gt;\n  select(category, CounDist, total_trees, dead_trees, Shape_Area, tree_density, dead_frac)\n\nkable(key_districts)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory\nCounDist\ntotal_trees\ndead_trees\nShape_Area\ntree_density\ndead_frac\n\n\n\n\nMost Trees\n50\n3332\n571\n665196534\n5.0e-06\n0.1713685\n\n\nHighest Density\n35\n494\n80\n79440619\n6.2e-06\n0.1619433\n\n\nHighest Dead Fraction\n16\n80\n48\n62082481\n1.3e-06\n0.6000000\n\n\n\n\n\n‚úÖ The top districts in each category."
  },
  {
    "objectID": "mp03.html#most-common-tree-species-in-manhattan",
    "href": "mp03.html#most-common-tree-species-in-manhattan",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "4.4 Most common tree species in Manhattan",
    "text": "4.4 Most common tree species in Manhattan\n\n\nCode\ntrees_joined &lt;- trees_joined %&gt;%\n  mutate(\n    borough = case_when(\n      CounDist &gt;= 1  & CounDist &lt;= 10 ~ \"Manhattan\",\n      CounDist &gt;= 11 & CounDist &lt;= 18 ~ \"Bronx\",\n      CounDist &gt;= 19 & CounDist &lt;= 32 ~ \"Queens\",\n      CounDist &gt;= 33 & CounDist &lt;= 48 ~ \"Brooklyn\",\n      CounDist &gt;= 49 & CounDist &lt;= 51 ~ \"Staten Island\",\n      TRUE ~ NA_character_\n    )\n  )\nmost_common_manhattan &lt;- trees_joined %&gt;%\n  filter(borough == \"Manhattan\") %&gt;%\n  st_drop_geometry() %&gt;%\n  count(genusspecies, sort = TRUE) %&gt;%\n  slice(1)\n\nmost_common_manhattan\n\n\n                                                genusspecies   n\n1 Gleditsia triacanthos var. inermis - Thornless honeylocust 399\n\n\n\nTop 10 Most Common Species in Manhattan (Bar Plot)\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Top 10 species in Manhattan\ntop10_manhattan &lt;- trees_joined %&gt;%\n  filter(borough == \"Manhattan\") %&gt;%\n  st_drop_geometry() %&gt;%\n  count(genusspecies, sort = TRUE) %&gt;%\n  slice_head(n = 10)\n\ntop10_manhattan\n\n\n                                                 genusspecies   n\n1  Gleditsia triacanthos var. inermis - Thornless honeylocust 399\n2                             Pyrus calleryana - Callery pear 218\n3                    Platanus x acerifolia - London planetree 215\n4                          Zelkova serrata - Japanese zelkova 179\n5              Styphnolobium japonicum - Japanese pagoda tree 173\n6                           Tilia cordata - littleleaf linden 165\n7                             Ginkgo biloba - maidenhair tree 125\n8                                 Quercus palustris - pin oak 114\n9                                           Unknown - Unknown  39\n10                             Ulmus americana - American elm  37\n\n\nCode\n# Bar plot\nggplot(top10_manhattan, aes(x = n, y = reorder(genusspecies, n))) +\n  geom_col(fill = \"#2E86C1\") +\n  labs(\n    title = \"Top 10 Most Common Tree Species in Manhattan\",\n    x = \"Number of Trees\",\n    y = \"Species\"\n  ) +\n  theme_minimal(base_size = 12)"
  },
  {
    "objectID": "mp03.html#title-replant-revive-thornless-honeylocust-initiative-in-district-4-midtown-east",
    "href": "mp03.html#title-replant-revive-thornless-honeylocust-initiative-in-district-4-midtown-east",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "Title: ‚ÄúReplant & Revive: Thornless Honeylocust Initiative in District 4 (Midtown East)‚Äù",
    "text": "Title: ‚ÄúReplant & Revive: Thornless Honeylocust Initiative in District 4 (Midtown East)‚Äù\n\nDistrict 4 Overview:\nCouncil District 4 covers Midtown East, Murray Hill, and Kips Bay ‚Äî a dense commercial and residential area that includes Baruch College. While the district has a healthy urban canopy, data from the Forestry Tree Points sample shows that it has one of the lower tree densities relative to its land area compared to nearby Manhattan districts. Increasing tree cover here would improve shade, pedestrian comfort, and air quality in heavily trafficked zones."
  },
  {
    "objectID": "mp03.html#proposed-project-description",
    "href": "mp03.html#proposed-project-description",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "Proposed Project Description",
    "text": "Proposed Project Description\nWe propose the Replant & Revive program, a one-year initiative led by the NYC Parks Department to replace dead and stump trees and add new Thornless Honeylocust ( Gleditsia triacanthos var. inermis ) trees along East 23rd to 30th Streets. This species thrives in compact soils and high-traffic areas, making it ideal for District 4‚Äôs urban environment.\n\nScope of Work:\n\nPlant 250 new Honeylocust trees\nRemove 60 dead or stump trees (as identified by tpcondition == ‚ÄúDead‚Äù or ‚ÄúStump‚Äù)\nProvide two-year maintenance (watering & pruning) to ensure survival"
  },
  {
    "objectID": "mp03.html#why-district-4",
    "href": "mp03.html#why-district-4",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "Why District 4?",
    "text": "Why District 4?\n\n1Ô∏è‚É£ Zoomed-in Map of District 4 With All Trees\n\n\nCode\ndistrict4 &lt;- districts %&gt;% filter(CounDist == 4)\n\ntrees_d4 &lt;- trees_joined %&gt;% filter(CounDist == 4)\n\nggplot() +\ngeom_sf(data = district4, fill = \"lightyellow\", color = \"black\") +\ngeom_sf(data = trees_d4, color = \"darkgreen\", alpha = 0.5, size = 0.8) +\nlabs(\ntitle = \"Tree Distribution in District 4\",\nsubtitle = \"Zoomed-In View Showing All Trees\",\ncaption = \"Source: NYC Open Data\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nCompared with peer Manhattan districts:\n\n\n\n\n\n\n\n\n\nDistrict\nTotal Trees\nDead Tree %\nTree Density (per sq. meter)\n\n\n\n\nDistrict 4\n316\nHigh\nLower mid range\n\n\nDistrict 3\nHigher\nLower\nHigher\n\n\nDistrict 6\nHigher\nLower\nHigher\n\n\nDistrict 10\nMuch Higher\nLower\nHighest\n\n\n\nDistrict 4 shows one of the highest proportions of declining-condition trees relative to its size and a lower canopy density compared with adjacent districts.\n\n\nMap-Based Comparison of District 4 vs District 10\n\n\nCode\nd4 &lt;- districts %&gt;% filter(CounDist == 4)\nd10 &lt;- districts %&gt;% filter(CounDist == 10)\n\ntrees_d10 &lt;- trees_joined %&gt;% filter(CounDist == 10)\n\nggplot() +\ngeom_sf(data = d4, fill = \"lightblue\", alpha = 0.4) +\ngeom_sf(data = trees_d4, color = \"green\", size = 0.4, alpha = 0.5) +\ngeom_sf(data = d10, fill = \"pink\", alpha = 0.4) +\ngeom_sf(data = trees_d10, color = \"red\", size = 0.4, alpha = 0.5) +\nlabs(\ntitle = \"District 4 vs District 10 ‚Äî Tree Coverage\",\nsubtitle = \"District 10 has significantly denser canopy coverage\",\ncaption = \"NYC Open Data\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nTherefore, District 4 benefits most from targeted replanting and maintenance."
  },
  {
    "objectID": "mp03.html#conclusion",
    "href": "mp03.html#conclusion",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "Conclusion",
    "text": "Conclusion\nThis proposal supports NYC Parks‚Äô mission to expand equitable green spaces. By improving District 4‚Äôs street trees, we create a healthier, cooler, and more welcoming neighborhood for residents, commuters, and visitors."
  },
  {
    "objectID": "mp03.html#tree-closest-to-baruch-college",
    "href": "mp03.html#tree-closest-to-baruch-college",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "üìç4.5 Tree Closest to Baruch College",
    "text": "üìç4.5 Tree Closest to Baruch College\n\n\nCode\n# Define a helper to make a point from latitude and longitude\nnew_st_point &lt;- function(lat, lon) {\n  st_sfc(st_point(c(lon, lat)), crs = \"WGS84\")\n}\n\n# Baruch College coordinates (25th Street & Lexington Ave)\nbaruch_pt &lt;- new_st_point(lat = 40.740173, lon = -73.98337)\n\n# Compute distances from each tree to Baruch\ntrees_joined &lt;- trees_joined |&gt;\n  mutate(distance = as.numeric(st_distance(geometry, baruch_pt)))\n\n# Find the single closest tree\nclosest_baruch &lt;- trees_joined |&gt;\n  arrange(distance) |&gt;\n  slice(1) |&gt;\n  st_drop_geometry() |&gt;\n  select(genusspecies, tpcondition, distance)\n\nclosest_baruch\n\n\n                     genusspecies tpcondition distance\n1 Pyrus calleryana - Callery pear        Fair 112.8705\n\n\nThe tree located closest to Baruch College (40.7401¬∞ N, ‚àí73.9833¬∞ W) is a Callery Pear (Pyrus calleryana), in fair condition, approximately 113 meters away. Although not a native species, Callery Pears are commonly used as ornamental street trees in dense neighborhoods like Midtown due to their symmetrical shape and white spring blossoms.\n\nüß†Extra Visualization\n\n\nCode\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"grey70\") +\n  geom_sf(data = trees_sf, color = \"darkgreen\", alpha = 0.05, size = 0.2) +\n  geom_sf(data = baruch_pt, color = \"red\", size = 2) +\n  labs(\n    title = \"Tree Closest to Baruch College\",\n    subtitle = \"Red dot marks Baruch College; green dots represent trees\",\n    caption = \"Source: NYC Open Data ‚Äì Forestry Tree Points\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "mp03.html#expected-impact",
    "href": "mp03.html#expected-impact",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "Expected Impact",
    "text": "Expected Impact\n\nIncrease canopy coverage by ‚âà 12 % in District 4 within two years\nImprove local air quality and cooling by adding shade near sidewalks\nSupport urban biodiversity through consistent species selection\nEngage residents and students through a ‚ÄúCommunity Plant-Day‚Äù led by Baruch College‚Äôs Sustainability Club"
  },
  {
    "objectID": "mp03.html#most-trees",
    "href": "mp03.html#most-trees",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "4.1 Most trees",
    "text": "4.1 Most trees\n\n\nCode\nmost_trees_dist &lt;- by_dist %&gt;%\n  arrange(desc(total_trees)) %&gt;%\n  slice(1) %&gt;%\n  select(CounDist, total_trees)\n\nmost_trees_dist\n\n\n# A tibble: 1 √ó 2\n  CounDist total_trees\n     &lt;int&gt;       &lt;int&gt;\n1       50        3332"
  },
  {
    "objectID": "mp03.html#highest-density-of-trees",
    "href": "mp03.html#highest-density-of-trees",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "4.2 Highest density of trees",
    "text": "4.2 Highest density of trees\n\n\nCode\nhighest_density_dist &lt;- by_dist %&gt;%\n  arrange(desc(tree_density)) %&gt;%\n  slice(1) %&gt;%\n  select(CounDist, tree_density)\n\nhighest_density_dist\n\n\n# A tibble: 1 √ó 2\n  CounDist tree_density\n     &lt;int&gt;        &lt;dbl&gt;\n1       35   0.00000622"
  },
  {
    "objectID": "mp03.html#highest-fraction-of-dead-trees",
    "href": "mp03.html#highest-fraction-of-dead-trees",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "4.3 Highest fraction of dead trees",
    "text": "4.3 Highest fraction of dead trees\n\n\nCode\nhighest_dead_frac_dist &lt;- by_dist %&gt;%\n  filter(total_trees &gt; 50) %&gt;%\n  arrange(desc(dead_frac)) %&gt;%\n  slice(1) %&gt;%\n  select(CounDist, dead_frac)\n\nhighest_dead_frac_dist\n\n\n# A tibble: 1 √ó 2\n  CounDist dead_frac\n     &lt;int&gt;     &lt;dbl&gt;\n1       16       0.6"
  },
  {
    "objectID": "mp03.html#tree-closest-to-baruchs-campus",
    "href": "mp03.html#tree-closest-to-baruchs-campus",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "4.5 Tree closest to Baruch‚Äôs campus",
    "text": "4.5 Tree closest to Baruch‚Äôs campus\n\n\nCode\n# Define a helper to make a point from latitude and longitude\nnew_st_point &lt;- function(lat, lon) {\n  st_sfc(st_point(c(lon, lat)), crs = \"WGS84\")\n}\n\n# Baruch College coordinates (25th Street & Lexington Ave)\nbaruch_pt &lt;- new_st_point(lat = 40.740173, lon = -73.98337)\n\n# Compute distances from each tree to Baruch\ntrees_joined &lt;- trees_joined |&gt;\n  mutate(distance = as.numeric(st_distance(geometry, baruch_pt)))\n\n# Find the single closest tree\nclosest_baruch &lt;- trees_joined |&gt;\n  arrange(distance) |&gt;\n  slice(1) |&gt;\n  st_drop_geometry() |&gt;\n  select(genusspecies, tpcondition, distance)\n\nclosest_baruch\n\n\n                     genusspecies tpcondition distance\n1 Pyrus calleryana - Callery pear        Fair 112.8705\n\n\n\nVisualization Tree closest to Baruch‚Äôs campus\n\n\nCode\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"grey70\") +\n  geom_sf(data = trees_sf, color = \"darkgreen\", alpha = 0.05, size = 0.2) +\n  geom_sf(data = baruch_pt, color = \"red\", size = 2) +\n  labs(\n    title = \"Tree Closest to Baruch College\",\n    subtitle = \"Red dot marks Baruch College; green dots represent trees\",\n    caption = \"Source: NYC Open Data ‚Äì Forestry Tree Points\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "mp03.html#overview",
    "href": "mp03.html#overview",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "Overview",
    "text": "Overview\nNew York City‚Äôs District 4 (Midtown East, Murray Hill, Kips Bay) has one of Manhattan‚Äôs highest pedestrian use areas but comparatively lower healthy tree density. Many locations have aging trees, declining health ratings, or residual stumps that reduce canopy coverage and shade availability.\nThis proposal recommends a District-focused maintenance and replanting program to strengthen canopy equity, reduce heat exposure, and improve neighborhood sustainability."
  },
  {
    "objectID": "mp03.html#proposed-project",
    "href": "mp03.html#proposed-project",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "Proposed Project",
    "text": "Proposed Project\n‚Äú45 Healthy Trees for District 4‚Äù Initiative, which includes:\n\nRemoving 30 damaged or dead trees (based on tpcondition assessment).\nPlanting 45 new street trees in priority locations near schools, senior centers, and high-traffic pedestrian corridors.\nProviding risk-rating inspections for ~200 existing trees marked as ‚ÄúFair‚Äù condition."
  },
  {
    "objectID": "mp03.html#why-this-project-matters",
    "href": "mp03.html#why-this-project-matters",
    "title": "Mini-Project #03: Visualizing & Maintaining NYC‚Äôs Green Canopy",
    "section": "Why This Project Matters",
    "text": "Why This Project Matters\n\nIncreases shade and reduces summer heat exposure\nSupports biodiversity with new species\nImproves air quality in a high-traffic zone\nEnhances visual appeal and pedestrian comfort\nBuilds long-term resilience in NYC‚Äôs urban forest"
  }
]